# Advanced Model Training, Tuning and Evaluation

## Week 1:
[C3_W1_Assignment](https://github.com/curtpond/practical-aws/blob/main/nb/week3/C3_W1_Assignment.ipynb)

[Slides for Week 1](./slides/C3_W1.pdf)
### Objectives
- Describe practical challenges related to data for machine learning - scale, resources (storage, compute, memory)
- Explain approaches to address challenges with data for machine learning
- Describe practical challenges related to ML training such as large data sets, optimizing training time & costs
- Explain approaches to address ML training challenges using techniques such as streaming data, profiling training infrastructure monitoring, data parallelism etc.
- Describe the concept of hyper-parameter tuning
- Explain popular algorithms used for automatic model tuning
- Apply hyper-parameter tuning to a BERT-based text classifier and dataset
- Describe the concept of model evaluation
- Demonstrate how to evaluate a model

## Week 2:
[C3_W2_Assignment](https://github.com/curtpond/practical-aws/blob/main/nb/week3/C3_W2_Assignment.ipynb)

[Slides for Week 2](./slides/C3_W2.pdf)
### Objectives
- Describe practical challenges related to ML deployment & monitoring
- Explain approaches to address deployment and monitoring challenges using techniques such as autoscaling, automating model quality monitoring, automating retraining pipelines
- Evaluate model deployment option and considerations
- Describe common deployment challenges and strategies for models
- Explain the considerations for monitoring a machine learning workload
- Describe approaches to model monitoring
- Discuss how to use Amazon SageMaker to perform A/B testing
- Demonstrate deploying a model version to Amazon SageMaker using A/B testing
